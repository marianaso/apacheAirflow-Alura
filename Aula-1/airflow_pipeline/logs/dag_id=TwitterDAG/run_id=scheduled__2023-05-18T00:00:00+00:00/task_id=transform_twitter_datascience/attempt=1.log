[2023-05-19 16:39:12,869] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 16:39:12,886] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 16:39:12,886] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 16:39:12,886] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-19 16:39:12,886] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 16:39:12,909] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-18 00:00:00+00:00
[2023-05-19 16:39:12,913] {standard_task_runner.py:52} INFO - Started process 6424 to run task
[2023-05-19 16:39:12,921] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-18T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpdnr7a1mz', '--error-file', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpm0i72d6a']
[2023-05-19 16:39:12,924] {standard_task_runner.py:80} INFO - Job 12: Subtask transform_twitter_datascience
[2023-05-19 16:39:13,019] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [running]> on host 1.0.0.127.in-addr.arpa
[2023-05-19 16:39:13,125] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-18T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-18T00:00:00+00:00
[2023-05-19 16:39:13,137] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-19 16:39:13,140] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience --dest /Users/marianaoliveira/Downloads/alura1/data_transformation --process-date 2023-05-18
[2023-05-19 16:39:17,755] {spark_submit.py:490} INFO - 23/05/19 16:39:17 WARN Utils: Your hostname, MacBook-Air-de-Mariana-2.local resolves to a loopback address: 127.0.0.1; using 192.168.15.8 instead (on interface en0)
[2023-05-19 16:39:17,771] {spark_submit.py:490} INFO - 23/05/19 16:39:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-19 16:39:20,174] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SparkContext: Running Spark version 3.3.2
[2023-05-19 16:39:20,390] {spark_submit.py:490} INFO - 23/05/19 16:39:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-19 16:39:20,694] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceUtils: ==============================================================
[2023-05-19 16:39:20,695] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-19 16:39:20,696] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceUtils: ==============================================================
[2023-05-19 16:39:20,699] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-19 16:39:20,746] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-19 16:39:20,759] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceProfile: Limiting resource is cpu
[2023-05-19 16:39:20,765] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-19 16:39:20,892] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SecurityManager: Changing view acls to: marianaoliveira
[2023-05-19 16:39:20,893] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SecurityManager: Changing modify acls to: marianaoliveira
[2023-05-19 16:39:20,894] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SecurityManager: Changing view acls groups to:
[2023-05-19 16:39:20,896] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SecurityManager: Changing modify acls groups to:
[2023-05-19 16:39:20,897] {spark_submit.py:490} INFO - 23/05/19 16:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marianaoliveira); groups with view permissions: Set(); users  with modify permissions: Set(marianaoliveira); groups with modify permissions: Set()
[2023-05-19 16:39:21,925] {spark_submit.py:490} INFO - 23/05/19 16:39:21 INFO Utils: Successfully started service 'sparkDriver' on port 53437.
[2023-05-19 16:39:22,063] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO SparkEnv: Registering MapOutputTracker
[2023-05-19 16:39:22,205] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-19 16:39:22,277] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-19 16:39:22,315] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-19 16:39:22,334] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-19 16:39:22,447] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO DiskBlockManager: Created local directory at /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/blockmgr-6f6a3402-b33d-48b5-81ad-330f2f4d30b3
[2023-05-19 16:39:22,502] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-19 16:39:22,555] {spark_submit.py:490} INFO - 23/05/19 16:39:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-19 16:39:23,403] {spark_submit.py:490} INFO - 23/05/19 16:39:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-19 16:39:23,453] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-19 16:39:23,865] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO Executor: Starting executor ID driver on host 192.168.15.8
[2023-05-19 16:39:23,885] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-19 16:39:23,961] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53440.
[2023-05-19 16:39:23,965] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO NettyBlockTransferService: Server created on 192.168.15.8:53440
[2023-05-19 16:39:23,968] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-19 16:39:23,986] {spark_submit.py:490} INFO - 23/05/19 16:39:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.15.8, 53440, None)
[2023-05-19 16:39:24,000] {spark_submit.py:490} INFO - 23/05/19 16:39:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.15.8:53440 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.15.8, 53440, None)
[2023-05-19 16:39:24,013] {spark_submit.py:490} INFO - 23/05/19 16:39:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.15.8, 53440, None)
[2023-05-19 16:39:24,017] {spark_submit.py:490} INFO - 23/05/19 16:39:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.15.8, 53440, None)
[2023-05-19 16:39:25,475] {spark_submit.py:490} INFO - 23/05/19 16:39:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-19 16:39:25,498] {spark_submit.py:490} INFO - 23/05/19 16:39:25 INFO SharedState: Warehouse path is 'file:/Users/marianaoliveira/Downloads/alura1/spark-warehouse'.
[2023-05-19 16:39:28,021] {spark_submit.py:490} INFO - 23/05/19 16:39:28 INFO InMemoryFileIndex: It took 141 ms to list leaf files for 1 paths.
[2023-05-19 16:39:28,391] {spark_submit.py:490} INFO - 23/05/19 16:39:28 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 2 paths.
[2023-05-19 16:39:34,564] {spark_submit.py:490} INFO - 23/05/19 16:39:34 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 16:39:34,565] {spark_submit.py:490} INFO - 23/05/19 16:39:34 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 16:39:34,578] {spark_submit.py:490} INFO - 23/05/19 16:39:34 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-19 16:39:35,358] {spark_submit.py:490} INFO - 23/05/19 16:39:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 207.4 KiB, free 434.2 MiB)
[2023-05-19 16:39:36,063] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
[2023-05-19 16:39:36,068] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.15.8:53440 (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 16:39:36,082] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:36,106] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8397722 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 16:39:36,592] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:36,629] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 16:39:36,629] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 16:39:36,630] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 16:39:36,632] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Missing parents: List()
[2023-05-19 16:39:36,638] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 16:39:36,854] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.1 MiB)
[2023-05-19 16:39:36,879] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.1 MiB)
[2023-05-19 16:39:36,884] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.15.8:53440 (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 16:39:36,898] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-19 16:39:36,943] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 16:39:36,944] {spark_submit.py:490} INFO - 23/05/19 16:39:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-19 16:39:37,063] {spark_submit.py:490} INFO - 23/05/19 16:39:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5167 bytes) taskResourceAssignments Map()
[2023-05-19 16:39:37,087] {spark_submit.py:490} INFO - 23/05/19 16:39:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-19 16:39:37,316] {spark_submit.py:490} INFO - 23/05/19 16:39:37 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-4572, partition values: [empty row]
[2023-05-19 16:39:37,907] {spark_submit.py:490} INFO - 23/05/19 16:39:37 INFO CodeGenerator: Code generated in 516.232704 ms
[2023-05-19 16:39:38,041] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4542, partition values: [empty row]
[2023-05-19 16:39:38,091] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-19 16:39:38,104] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1064 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 16:39:38,108] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-19 16:39:38,119] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.449 s
[2023-05-19 16:39:38,124] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 16:39:38,125] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-19 16:39:38,129] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.536420 s
[2023-05-19 16:39:38,496] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.15.8:53440 in memory (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 16:39:38,504] {spark_submit.py:490} INFO - 23/05/19 16:39:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.15.8:53440 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 16:39:39,310] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO DataSourceStrategy: Pruning directories with:
[2023-05-19 16:39:39,321] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-19 16:39:39,323] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-19 16:39:39,324] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-19 16:39:39,516] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 16:39:39,517] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 16:39:39,518] {spark_submit.py:490} INFO - 23/05/19 16:39:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 16:39:40,194] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO CodeGenerator: Code generated in 369.469036 ms
[2023-05-19 16:39:40,221] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 207.3 KiB, free 434.2 MiB)
[2023-05-19 16:39:40,295] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 16:39:40,296] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.15.8:53440 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 16:39:40,310] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:40,321] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8397722 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 16:39:40,557] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:40,562] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 16:39:40,562] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 16:39:40,563] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 16:39:40,564] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Missing parents: List()
[2023-05-19 16:39:40,565] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 16:39:40,659] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 240.2 KiB, free 433.9 MiB)
[2023-05-19 16:39:40,676] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.0 KiB, free 433.8 MiB)
[2023-05-19 16:39:40,678] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.15.8:53440 (size: 84.0 KiB, free: 434.3 MiB)
[2023-05-19 16:39:40,679] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-19 16:39:40,680] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 16:39:40,681] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-19 16:39:40,687] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5427 bytes) taskResourceAssignments Map()
[2023-05-19 16:39:40,688] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-19 16:39:40,807] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 16:39:40,808] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 16:39:40,808] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 16:39:40,900] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-4572, partition values: [19494]
[2023-05-19 16:39:40,965] {spark_submit.py:490} INFO - 23/05/19 16:39:40 INFO CodeGenerator: Code generated in 53.910625 ms
[2023-05-19 16:39:41,020] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO CodeGenerator: Code generated in 14.555024 ms
[2023-05-19 16:39:41,106] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4542, partition values: [19495]
[2023-05-19 16:39:41,144] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileOutputCommitter: Saved output of task 'attempt_202305191639401889025561410136110_0001_m_000000_1' to file:/Users/marianaoliveira/Downloads/alura1/data_transformation/tweet/process_date=2023-05-18/_temporary/0/task_202305191639401889025561410136110_0001_m_000000
[2023-05-19 16:39:41,146] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO SparkHadoopMapRedUtil: attempt_202305191639401889025561410136110_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-05-19 16:39:41,162] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-19 16:39:41,166] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 483 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 16:39:41,167] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-19 16:39:41,169] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.602 s
[2023-05-19 16:39:41,171] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 16:39:41,172] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-19 16:39:41,175] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.615600 s
[2023-05-19 16:39:41,178] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileFormatWriter: Start to commit write Job d6255daa-0982-4281-a855-69fcd413ea99.
[2023-05-19 16:39:41,240] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileFormatWriter: Write Job d6255daa-0982-4281-a855-69fcd413ea99 committed. Elapsed time: 60 ms.
[2023-05-19 16:39:41,260] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileFormatWriter: Finished processing stats for write job d6255daa-0982-4281-a855-69fcd413ea99.
[2023-05-19 16:39:41,422] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DataSourceStrategy: Pruning directories with:
[2023-05-19 16:39:41,424] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 16:39:41,425] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 16:39:41,426] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-19 16:39:41,448] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 16:39:41,449] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 16:39:41,451] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 16:39:41,567] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO CodeGenerator: Code generated in 39.052099 ms
[2023-05-19 16:39:41,584] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 207.3 KiB, free 433.6 MiB)
[2023-05-19 16:39:41,615] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.15.8:53440 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-19 16:39:41,620] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.15.8:53440 in memory (size: 84.0 KiB, free: 434.4 MiB)
[2023-05-19 16:39:41,652] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 16:39:41,653] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.15.8:53440 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 16:39:41,660] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:41,663] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8397722 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 16:39:41,810] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 16:39:41,812] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 16:39:41,812] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 16:39:41,813] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 16:39:41,814] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Missing parents: List()
[2023-05-19 16:39:41,815] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 16:39:41,903] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 217.2 KiB, free 434.0 MiB)
[2023-05-19 16:39:41,970] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
[2023-05-19 16:39:41,978] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.15.8:53440 (size: 77.5 KiB, free: 434.3 MiB)
[2023-05-19 16:39:41,980] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-19 16:39:41,981] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 16:39:41,982] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-19 16:39:41,984] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5427 bytes) taskResourceAssignments Map()
[2023-05-19 16:39:41,985] {spark_submit.py:490} INFO - 23/05/19 16:39:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-19 16:39:42,051] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 16:39:42,051] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 16:39:42,052] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 16:39:42,118] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-4572, partition values: [19494]
[2023-05-19 16:39:42,153] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO CodeGenerator: Code generated in 24.974951 ms
[2023-05-19 16:39:42,162] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4542, partition values: [19495]
[2023-05-19 16:39:42,183] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileOutputCommitter: Saved output of task 'attempt_202305191639418314467104146715295_0002_m_000000_2' to file:/Users/marianaoliveira/Downloads/alura1/data_transformation/user/process_date=2023-05-18/_temporary/0/task_202305191639418314467104146715295_0002_m_000000
[2023-05-19 16:39:42,184] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO SparkHadoopMapRedUtil: attempt_202305191639418314467104146715295_0002_m_000000_2: Committed. Elapsed time: 5 ms.
[2023-05-19 16:39:42,186] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-19 16:39:42,193] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 209 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 16:39:42,194] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-19 16:39:42,195] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.380 s
[2023-05-19 16:39:42,196] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 16:39:42,197] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-19 16:39:42,198] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.385763 s
[2023-05-19 16:39:42,199] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileFormatWriter: Start to commit write Job 317cd351-9c37-4285-8930-bd34c77f16eb.
[2023-05-19 16:39:42,234] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileFormatWriter: Write Job 317cd351-9c37-4285-8930-bd34c77f16eb committed. Elapsed time: 35 ms.
[2023-05-19 16:39:42,235] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO FileFormatWriter: Finished processing stats for write job 317cd351-9c37-4285-8930-bd34c77f16eb.
[2023-05-19 16:39:42,307] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-19 16:39:42,336] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO SparkUI: Stopped Spark web UI at http://192.168.15.8:4041
[2023-05-19 16:39:42,360] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-19 16:39:42,392] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO MemoryStore: MemoryStore cleared
[2023-05-19 16:39:42,393] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO BlockManager: BlockManager stopped
[2023-05-19 16:39:42,398] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-19 16:39:42,403] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-19 16:39:42,418] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO SparkContext: Successfully stopped SparkContext
[2023-05-19 16:39:42,418] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO ShutdownHookManager: Shutdown hook called
[2023-05-19 16:39:42,419] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-f6f3cc95-bef0-4b48-b3d7-bb9f9b51a1ee
[2023-05-19 16:39:42,429] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-def8eea9-b701-4e61-8b4d-075ad096d06a
[2023-05-19 16:39:42,441] {spark_submit.py:490} INFO - 23/05/19 16:39:42 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-def8eea9-b701-4e61-8b4d-075ad096d06a/pyspark-9ce29323-252e-46b7-8bc2-071bc6eb34f6
[2023-05-19 16:39:42,560] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230518T000000, start_date=20230519T193912, end_date=20230519T193942
[2023-05-19 16:39:42,624] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-19 16:39:42,653] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-19 16:54:51,874] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 16:54:51,903] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 16:54:51,903] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 16:54:51,903] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-19 16:54:51,903] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 16:54:51,953] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-18 00:00:00+00:00
[2023-05-19 16:54:51,967] {standard_task_runner.py:52} INFO - Started process 6923 to run task
[2023-05-19 16:54:51,971] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-18T00:00:00+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmp45e8_8ab', '--error-file', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpm14tgq8n']
[2023-05-19 16:54:51,974] {standard_task_runner.py:80} INFO - Job 18: Subtask transform_twitter_datascience
[2023-05-19 16:54:52,051] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [running]> on host MacBook-Air-de-Mariana-2.local
[2023-05-19 16:54:52,142] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-18T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-18T00:00:00+00:00
[2023-05-19 16:54:52,153] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-19 16:54:52,156] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /datalake/bronze/twitter_datascience --dest /datalake/silver/twitter_datascience --process-date 2023-05-18
[2023-05-19 16:55:00,889] {spark_submit.py:490} INFO - 23/05/19 16:55:00 INFO SparkContext: Running Spark version 3.3.2
[2023-05-19 16:55:01,081] {spark_submit.py:490} INFO - 23/05/19 16:55:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-19 16:55:01,612] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceUtils: ==============================================================
[2023-05-19 16:55:01,613] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-19 16:55:01,614] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceUtils: ==============================================================
[2023-05-19 16:55:01,615] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-19 16:55:01,699] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-19 16:55:01,734] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceProfile: Limiting resource is cpu
[2023-05-19 16:55:01,736] {spark_submit.py:490} INFO - 23/05/19 16:55:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-19 16:55:02,200] {spark_submit.py:490} INFO - 23/05/19 16:55:02 INFO SecurityManager: Changing view acls to: marianaoliveira
[2023-05-19 16:55:02,201] {spark_submit.py:490} INFO - 23/05/19 16:55:02 INFO SecurityManager: Changing modify acls to: marianaoliveira
[2023-05-19 16:55:02,202] {spark_submit.py:490} INFO - 23/05/19 16:55:02 INFO SecurityManager: Changing view acls groups to:
[2023-05-19 16:55:02,204] {spark_submit.py:490} INFO - 23/05/19 16:55:02 INFO SecurityManager: Changing modify acls groups to:
[2023-05-19 16:55:02,211] {spark_submit.py:490} INFO - 23/05/19 16:55:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marianaoliveira); groups with view permissions: Set(); users  with modify permissions: Set(marianaoliveira); groups with modify permissions: Set()
[2023-05-19 16:55:03,466] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO Utils: Successfully started service 'sparkDriver' on port 53819.
[2023-05-19 16:55:03,585] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO SparkEnv: Registering MapOutputTracker
[2023-05-19 16:55:03,717] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-19 16:55:03,804] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-19 16:55:03,806] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-19 16:55:03,819] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-19 16:55:03,910] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO DiskBlockManager: Created local directory at /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/blockmgr-1c32e9d3-4290-46a3-acd2-d1553a09d2e7
[2023-05-19 16:55:03,972] {spark_submit.py:490} INFO - 23/05/19 16:55:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-19 16:55:04,033] {spark_submit.py:490} INFO - 23/05/19 16:55:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-19 16:55:04,639] {spark_submit.py:490} INFO - 23/05/19 16:55:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-19 16:55:04,662] {spark_submit.py:490} INFO - 23/05/19 16:55:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-19 16:55:05,003] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO Executor: Starting executor ID driver on host 192.168.15.8
[2023-05-19 16:55:05,021] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-19 16:55:05,071] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53820.
[2023-05-19 16:55:05,072] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO NettyBlockTransferService: Server created on 192.168.15.8:53820
[2023-05-19 16:55:05,078] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-19 16:55:05,096] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.15.8, 53820, None)
[2023-05-19 16:55:05,107] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.15.8:53820 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.15.8, 53820, None)
[2023-05-19 16:55:05,120] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.15.8, 53820, None)
[2023-05-19 16:55:05,123] {spark_submit.py:490} INFO - 23/05/19 16:55:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.15.8, 53820, None)
[2023-05-19 16:55:06,595] {spark_submit.py:490} INFO - 23/05/19 16:55:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-19 16:55:06,617] {spark_submit.py:490} INFO - 23/05/19 16:55:06 INFO SharedState: Warehouse path is 'file:/Users/marianaoliveira/Downloads/alura1/spark-warehouse'.
[2023-05-19 16:55:09,150] {spark_submit.py:490} INFO - Traceback (most recent call last):
[2023-05-19 16:55:09,151] {spark_submit.py:490} INFO - File "/Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py", line 47, in <module>
[2023-05-19 16:55:09,152] {spark_submit.py:490} INFO - twitter_transformation(spark, args.src, args.dest, args.process_date)
[2023-05-19 16:55:09,152] {spark_submit.py:490} INFO - File "/Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py", line 21, in twitter_transformation
[2023-05-19 16:55:09,153] {spark_submit.py:490} INFO - df = spark.read.json(src)
[2023-05-19 16:55:09,154] {spark_submit.py:490} INFO - File "/Users/marianaoliveira/Downloads/alura1/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 284, in json
[2023-05-19 16:55:09,155] {spark_submit.py:490} INFO - File "/Users/marianaoliveira/Downloads/alura1/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-05-19 16:55:09,155] {spark_submit.py:490} INFO - File "/Users/marianaoliveira/Downloads/alura1/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-05-19 16:55:09,165] {spark_submit.py:490} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: file:/datalake/bronze/twitter_datascience
[2023-05-19 16:55:09,263] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-19 16:55:09,287] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO SparkUI: Stopped Spark web UI at http://192.168.15.8:4041
[2023-05-19 16:55:09,314] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-19 16:55:09,340] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO MemoryStore: MemoryStore cleared
[2023-05-19 16:55:09,341] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO BlockManager: BlockManager stopped
[2023-05-19 16:55:09,373] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-19 16:55:09,417] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-19 16:55:09,487] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO SparkContext: Successfully stopped SparkContext
[2023-05-19 16:55:09,487] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO ShutdownHookManager: Shutdown hook called
[2023-05-19 16:55:09,488] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-f5738523-7159-4b1a-9ee0-ad19aceace39
[2023-05-19 16:55:09,503] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-4e7ce46d-3b32-4619-be74-d7ad950d578f
[2023-05-19 16:55:09,517] {spark_submit.py:490} INFO - 23/05/19 16:55:09 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-4e7ce46d-3b32-4619-be74-d7ad950d578f/pyspark-62057582-3a54-4938-b479-66cd8da6336f
[2023-05-19 16:55:09,718] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/marianaoliveira/Downloads/alura1/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/Users/marianaoliveira/Downloads/alura1/venv/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 421, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /datalake/bronze/twitter_datascience --dest /datalake/silver/twitter_datascience --process-date 2023-05-18. Error code is: 1.
[2023-05-19 16:55:09,729] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230518T000000, start_date=20230519T195451, end_date=20230519T195509
[2023-05-19 16:55:09,752] {standard_task_runner.py:92} ERROR - Failed to execute job 18 for task transform_twitter_datascience (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /datalake/bronze/twitter_datascience --dest /datalake/silver/twitter_datascience --process-date 2023-05-18. Error code is: 1.; 6923)
[2023-05-19 16:55:09,769] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-05-19 16:55:09,798] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-19 17:06:08,284] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 17:06:08,299] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 17:06:08,299] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 17:06:08,299] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-19 17:06:08,299] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 17:06:08,399] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-18 00:00:00+00:00
[2023-05-19 17:06:08,404] {standard_task_runner.py:52} INFO - Started process 7394 to run task
[2023-05-19 17:06:08,413] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-18T00:00:00+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpk8zqnkj7', '--error-file', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmp4j2uq0k1']
[2023-05-19 17:06:08,416] {standard_task_runner.py:80} INFO - Job 18: Subtask transform_twitter_datascience
[2023-05-19 17:06:08,491] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [running]> on host 1.0.0.127.in-addr.arpa
[2023-05-19 17:06:08,583] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-18T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-18T00:00:00+00:00
[2023-05-19 17:06:08,594] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-19 17:06:08,601] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src datalake/bronze/twitter_datascience --dest datalake/silver/twitter_datascience --process-date 2023-05-18
[2023-05-19 17:06:13,520] {spark_submit.py:490} INFO - 23/05/19 17:06:13 WARN Utils: Your hostname, MacBook-Air-de-Mariana-2.local resolves to a loopback address: 127.0.0.1; using 192.168.15.8 instead (on interface en0)
[2023-05-19 17:06:13,530] {spark_submit.py:490} INFO - 23/05/19 17:06:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-19 17:06:16,292] {spark_submit.py:490} INFO - 23/05/19 17:06:16 INFO SparkContext: Running Spark version 3.3.2
[2023-05-19 17:06:16,538] {spark_submit.py:490} INFO - 23/05/19 17:06:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-19 17:06:16,960] {spark_submit.py:490} INFO - 23/05/19 17:06:16 INFO ResourceUtils: ==============================================================
[2023-05-19 17:06:16,961] {spark_submit.py:490} INFO - 23/05/19 17:06:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-19 17:06:16,962] {spark_submit.py:490} INFO - 23/05/19 17:06:16 INFO ResourceUtils: ==============================================================
[2023-05-19 17:06:16,963] {spark_submit.py:490} INFO - 23/05/19 17:06:16 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-19 17:06:17,165] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-19 17:06:17,192] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO ResourceProfile: Limiting resource is cpu
[2023-05-19 17:06:17,201] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-19 17:06:17,361] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO SecurityManager: Changing view acls to: marianaoliveira
[2023-05-19 17:06:17,362] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO SecurityManager: Changing modify acls to: marianaoliveira
[2023-05-19 17:06:17,363] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO SecurityManager: Changing view acls groups to:
[2023-05-19 17:06:17,364] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO SecurityManager: Changing modify acls groups to:
[2023-05-19 17:06:17,364] {spark_submit.py:490} INFO - 23/05/19 17:06:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marianaoliveira); groups with view permissions: Set(); users  with modify permissions: Set(marianaoliveira); groups with modify permissions: Set()
[2023-05-19 17:06:18,675] {spark_submit.py:490} INFO - 23/05/19 17:06:18 INFO Utils: Successfully started service 'sparkDriver' on port 54095.
[2023-05-19 17:06:18,812] {spark_submit.py:490} INFO - 23/05/19 17:06:18 INFO SparkEnv: Registering MapOutputTracker
[2023-05-19 17:06:19,100] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-19 17:06:19,274] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-19 17:06:19,326] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-19 17:06:19,351] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-19 17:06:19,492] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO DiskBlockManager: Created local directory at /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/blockmgr-0685a6b1-d389-4960-af78-b72685192937
[2023-05-19 17:06:19,550] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-19 17:06:19,642] {spark_submit.py:490} INFO - 23/05/19 17:06:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-19 17:06:20,998] {spark_submit.py:490} INFO - 23/05/19 17:06:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-19 17:06:21,024] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-19 17:06:21,514] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO Executor: Starting executor ID driver on host 192.168.15.8
[2023-05-19 17:06:21,537] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-19 17:06:21,630] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54097.
[2023-05-19 17:06:21,631] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO NettyBlockTransferService: Server created on 192.168.15.8:54097
[2023-05-19 17:06:21,638] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-19 17:06:21,657] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.15.8, 54097, None)
[2023-05-19 17:06:21,670] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.15.8:54097 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.15.8, 54097, None)
[2023-05-19 17:06:21,682] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.15.8, 54097, None)
[2023-05-19 17:06:21,685] {spark_submit.py:490} INFO - 23/05/19 17:06:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.15.8, 54097, None)
[2023-05-19 17:06:23,752] {spark_submit.py:490} INFO - 23/05/19 17:06:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-19 17:06:23,784] {spark_submit.py:490} INFO - 23/05/19 17:06:23 INFO SharedState: Warehouse path is 'file:/Users/marianaoliveira/Downloads/alura1/spark-warehouse'.
[2023-05-19 17:06:29,328] {spark_submit.py:490} INFO - 23/05/19 17:06:29 INFO InMemoryFileIndex: It took 302 ms to list leaf files for 1 paths.
[2023-05-19 17:06:29,952] {spark_submit.py:490} INFO - 23/05/19 17:06:29 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 2 paths.
[2023-05-19 17:06:38,595] {spark_submit.py:490} INFO - 23/05/19 17:06:38 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 17:06:38,601] {spark_submit.py:490} INFO - 23/05/19 17:06:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 17:06:38,611] {spark_submit.py:490} INFO - 23/05/19 17:06:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-19 17:06:39,579] {spark_submit.py:490} INFO - 23/05/19 17:06:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 207.4 KiB, free 434.2 MiB)
[2023-05-19 17:06:39,987] {spark_submit.py:490} INFO - 23/05/19 17:06:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
[2023-05-19 17:06:39,992] {spark_submit.py:490} INFO - 23/05/19 17:06:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.15.8:54097 (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 17:06:40,012] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:40,062] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8402358 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 17:06:40,520] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:40,554] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 17:06:40,554] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 17:06:40,555] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 17:06:40,557] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Missing parents: List()
[2023-05-19 17:06:40,564] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 17:06:40,787] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.1 MiB)
[2023-05-19 17:06:40,802] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.1 MiB)
[2023-05-19 17:06:40,804] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.15.8:54097 (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 17:06:40,805] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-19 17:06:40,870] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 17:06:40,873] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-19 17:06:40,997] {spark_submit.py:490} INFO - 23/05/19 17:06:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5181 bytes) taskResourceAssignments Map()
[2023-05-19 17:06:41,020] {spark_submit.py:490} INFO - 23/05/19 17:06:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-19 17:06:41,260] {spark_submit.py:490} INFO - 23/05/19 17:06:41 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-9243, partition values: [empty row]
[2023-05-19 17:06:42,341] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO CodeGenerator: Code generated in 951.702238 ms
[2023-05-19 17:06:42,556] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4507, partition values: [empty row]
[2023-05-19 17:06:42,760] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-05-19 17:06:42,778] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1803 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 17:06:42,783] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-19 17:06:42,795] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 2.191 s
[2023-05-19 17:06:42,805] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 17:06:42,806] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-19 17:06:42,811] {spark_submit.py:490} INFO - 23/05/19 17:06:42 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 2.290211 s
[2023-05-19 17:06:43,303] {spark_submit.py:490} INFO - 23/05/19 17:06:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.15.8:54097 in memory (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 17:06:43,314] {spark_submit.py:490} INFO - 23/05/19 17:06:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.15.8:54097 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 17:06:44,193] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO DataSourceStrategy: Pruning directories with:
[2023-05-19 17:06:44,203] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-19 17:06:44,205] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-19 17:06:44,207] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-19 17:06:44,393] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 17:06:44,394] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 17:06:44,395] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 17:06:44,935] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO CodeGenerator: Code generated in 306.61045 ms
[2023-05-19 17:06:44,954] {spark_submit.py:490} INFO - 23/05/19 17:06:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 207.3 KiB, free 434.2 MiB)
[2023-05-19 17:06:45,036] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 17:06:45,039] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.15.8:54097 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 17:06:45,050] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:45,064] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8402358 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 17:06:45,317] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:45,321] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 17:06:45,322] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 17:06:45,323] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 17:06:45,324] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Missing parents: List()
[2023-05-19 17:06:45,324] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 17:06:45,431] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 240.3 KiB, free 433.9 MiB)
[2023-05-19 17:06:45,487] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.0 KiB, free 433.8 MiB)
[2023-05-19 17:06:45,495] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.15.8:54097 (size: 84.0 KiB, free: 434.3 MiB)
[2023-05-19 17:06:45,502] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-19 17:06:45,503] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 17:06:45,503] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-19 17:06:45,507] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5441 bytes) taskResourceAssignments Map()
[2023-05-19 17:06:45,508] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-19 17:06:45,647] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 17:06:45,652] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 17:06:45,653] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 17:06:45,818] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-9243, partition values: [19494]
[2023-05-19 17:06:45,872] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO CodeGenerator: Code generated in 51.806557 ms
[2023-05-19 17:06:45,936] {spark_submit.py:490} INFO - 23/05/19 17:06:45 INFO CodeGenerator: Code generated in 12.514507 ms
[2023-05-19 17:06:46,003] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4507, partition values: [19495]
[2023-05-19 17:06:46,037] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: Saved output of task 'attempt_202305191706455968991074233473343_0001_m_000000_1' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/tweet/process_date=2023-05-18/_temporary/0/task_202305191706455968991074233473343_0001_m_000000
[2023-05-19 17:06:46,039] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkHadoopMapRedUtil: attempt_202305191706455968991074233473343_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-05-19 17:06:46,050] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-19 17:06:46,053] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 551 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 17:06:46,054] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-19 17:06:46,055] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.730 s
[2023-05-19 17:06:46,056] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 17:06:46,058] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-19 17:06:46,059] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.739330 s
[2023-05-19 17:06:46,062] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Start to commit write Job ab6192eb-b5f0-4879-86a5-638b46f2507e.
[2023-05-19 17:06:46,107] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Write Job ab6192eb-b5f0-4879-86a5-638b46f2507e committed. Elapsed time: 42 ms.
[2023-05-19 17:06:46,112] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Finished processing stats for write job ab6192eb-b5f0-4879-86a5-638b46f2507e.
[2023-05-19 17:06:46,167] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DataSourceStrategy: Pruning directories with:
[2023-05-19 17:06:46,168] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 17:06:46,168] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 17:06:46,169] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-19 17:06:46,191] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 17:06:46,192] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 17:06:46,195] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 17:06:46,279] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO CodeGenerator: Code generated in 29.436431 ms
[2023-05-19 17:06:46,289] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 207.3 KiB, free 433.6 MiB)
[2023-05-19 17:06:46,310] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.15.8:54097 in memory (size: 84.0 KiB, free: 434.4 MiB)
[2023-05-19 17:06:46,315] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.15.8:54097 in memory (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 17:06:46,337] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 17:06:46,341] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.15.8:54097 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 17:06:46,347] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:46,349] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8402358 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 17:06:46,504] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 17:06:46,506] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 17:06:46,507] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 17:06:46,508] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 17:06:46,509] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Missing parents: List()
[2023-05-19 17:06:46,511] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 17:06:46,584] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 217.3 KiB, free 434.0 MiB)
[2023-05-19 17:06:46,591] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.5 KiB, free 433.9 MiB)
[2023-05-19 17:06:46,592] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.15.8:54097 (size: 77.5 KiB, free: 434.3 MiB)
[2023-05-19 17:06:46,598] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-19 17:06:46,599] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 17:06:46,600] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-19 17:06:46,602] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5441 bytes) taskResourceAssignments Map()
[2023-05-19 17:06:46,604] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-19 17:06:46,664] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 17:06:46,665] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 17:06:46,669] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 17:06:46,738] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-17/datascience_20230517.json, range: 0-9243, partition values: [19494]
[2023-05-19 17:06:46,775] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO CodeGenerator: Code generated in 33.946416 ms
[2023-05-19 17:06:46,804] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4507, partition values: [19495]
[2023-05-19 17:06:46,823] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileOutputCommitter: Saved output of task 'attempt_20230519170646459844664777342013_0002_m_000000_2' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/user/process_date=2023-05-18/_temporary/0/task_20230519170646459844664777342013_0002_m_000000
[2023-05-19 17:06:46,824] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkHadoopMapRedUtil: attempt_20230519170646459844664777342013_0002_m_000000_2: Committed. Elapsed time: 3 ms.
[2023-05-19 17:06:46,834] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-19 17:06:46,838] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 236 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 17:06:46,838] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-19 17:06:46,843] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.333 s
[2023-05-19 17:06:46,844] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 17:06:46,845] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-19 17:06:46,848] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.343528 s
[2023-05-19 17:06:46,850] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Start to commit write Job 81cca3d4-f127-43f6-b8de-a2045b86c0a7.
[2023-05-19 17:06:46,902] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Write Job 81cca3d4-f127-43f6-b8de-a2045b86c0a7 committed. Elapsed time: 52 ms.
[2023-05-19 17:06:46,904] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO FileFormatWriter: Finished processing stats for write job 81cca3d4-f127-43f6-b8de-a2045b86c0a7.
[2023-05-19 17:06:46,970] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-19 17:06:46,998] {spark_submit.py:490} INFO - 23/05/19 17:06:46 INFO SparkUI: Stopped Spark web UI at http://192.168.15.8:4041
[2023-05-19 17:06:47,020] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-19 17:06:47,054] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO MemoryStore: MemoryStore cleared
[2023-05-19 17:06:47,055] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO BlockManager: BlockManager stopped
[2023-05-19 17:06:47,061] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-19 17:06:47,065] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-19 17:06:47,079] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO SparkContext: Successfully stopped SparkContext
[2023-05-19 17:06:47,080] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO ShutdownHookManager: Shutdown hook called
[2023-05-19 17:06:47,081] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-0930ff2c-bdb6-438c-988c-e7f225b87d5c/pyspark-b02572be-e698-47a8-831d-0dcb85d022c2
[2023-05-19 17:06:47,094] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-3a256873-89a5-4f66-916c-89a9edc2d96c
[2023-05-19 17:06:47,100] {spark_submit.py:490} INFO - 23/05/19 17:06:47 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-0930ff2c-bdb6-438c-988c-e7f225b87d5c
[2023-05-19 17:06:47,213] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230518T000000, start_date=20230519T200608, end_date=20230519T200647
[2023-05-19 17:06:47,246] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-19 17:06:47,284] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-19 18:00:45,429] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 18:00:45,443] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 18:00:45,443] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 18:00:45,443] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-19 18:00:45,443] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 18:00:45,466] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-18 00:00:00+00:00
[2023-05-19 18:00:45,476] {standard_task_runner.py:52} INFO - Started process 8709 to run task
[2023-05-19 18:00:45,488] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-18T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpsybwdk2g', '--error-file', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpkgazefwl']
[2023-05-19 18:00:45,493] {standard_task_runner.py:80} INFO - Job 26: Subtask transform_twitter_datascience
[2023-05-19 18:00:45,566] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [running]> on host 1.0.0.127.in-addr.arpa
[2023-05-19 18:00:45,663] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-18T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-18T00:00:00+00:00
[2023-05-19 18:00:45,674] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-19 18:00:45,682] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18 --dest /Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/ --process-date 2023-05-18
[2023-05-19 18:00:50,425] {spark_submit.py:490} INFO - 23/05/19 18:00:50 WARN Utils: Your hostname, MacBook-Air-de-Mariana-2.local resolves to a loopback address: 127.0.0.1; using 192.168.15.8 instead (on interface en0)
[2023-05-19 18:00:50,432] {spark_submit.py:490} INFO - 23/05/19 18:00:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-19 18:00:52,954] {spark_submit.py:490} INFO - 23/05/19 18:00:52 INFO SparkContext: Running Spark version 3.3.2
[2023-05-19 18:00:53,201] {spark_submit.py:490} INFO - 23/05/19 18:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-19 18:00:53,638] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceUtils: ==============================================================
[2023-05-19 18:00:53,640] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-19 18:00:53,650] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceUtils: ==============================================================
[2023-05-19 18:00:53,667] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-19 18:00:53,759] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-19 18:00:53,781] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceProfile: Limiting resource is cpu
[2023-05-19 18:00:53,783] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-19 18:00:53,959] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SecurityManager: Changing view acls to: marianaoliveira
[2023-05-19 18:00:53,960] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SecurityManager: Changing modify acls to: marianaoliveira
[2023-05-19 18:00:53,961] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SecurityManager: Changing view acls groups to:
[2023-05-19 18:00:53,965] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SecurityManager: Changing modify acls groups to:
[2023-05-19 18:00:53,966] {spark_submit.py:490} INFO - 23/05/19 18:00:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marianaoliveira); groups with view permissions: Set(); users  with modify permissions: Set(marianaoliveira); groups with modify permissions: Set()
[2023-05-19 18:00:54,840] {spark_submit.py:490} INFO - 23/05/19 18:00:54 INFO Utils: Successfully started service 'sparkDriver' on port 54974.
[2023-05-19 18:00:54,950] {spark_submit.py:490} INFO - 23/05/19 18:00:54 INFO SparkEnv: Registering MapOutputTracker
[2023-05-19 18:00:55,062] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-19 18:00:55,137] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-19 18:00:55,147] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-19 18:00:55,155] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-19 18:00:55,224] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO DiskBlockManager: Created local directory at /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/blockmgr-e10474dd-a076-40db-87ad-cf06f4822a28
[2023-05-19 18:00:55,276] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-19 18:00:55,322] {spark_submit.py:490} INFO - 23/05/19 18:00:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-19 18:00:56,359] {spark_submit.py:490} INFO - 23/05/19 18:00:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-19 18:00:56,393] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-19 18:00:56,745] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO Executor: Starting executor ID driver on host 192.168.15.8
[2023-05-19 18:00:56,766] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-19 18:00:56,815] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54977.
[2023-05-19 18:00:56,816] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO NettyBlockTransferService: Server created on 192.168.15.8:54977
[2023-05-19 18:00:56,820] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-19 18:00:56,855] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.15.8, 54977, None)
[2023-05-19 18:00:56,875] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.15.8:54977 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.15.8, 54977, None)
[2023-05-19 18:00:56,880] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.15.8, 54977, None)
[2023-05-19 18:00:56,885] {spark_submit.py:490} INFO - 23/05/19 18:00:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.15.8, 54977, None)
[2023-05-19 18:00:58,343] {spark_submit.py:490} INFO - 23/05/19 18:00:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-19 18:00:58,363] {spark_submit.py:490} INFO - 23/05/19 18:00:58 INFO SharedState: Warehouse path is 'file:/Users/marianaoliveira/Downloads/alura1/spark-warehouse'.
[2023-05-19 18:01:01,489] {spark_submit.py:490} INFO - 23/05/19 18:01:01 INFO InMemoryFileIndex: It took 194 ms to list leaf files for 1 paths.
[2023-05-19 18:01:01,672] {spark_submit.py:490} INFO - 23/05/19 18:01:01 INFO InMemoryFileIndex: It took 16 ms to list leaf files for 1 paths.
[2023-05-19 18:01:08,626] {spark_submit.py:490} INFO - 23/05/19 18:01:08 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 18:01:08,629] {spark_submit.py:490} INFO - 23/05/19 18:01:08 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 18:01:08,637] {spark_submit.py:490} INFO - 23/05/19 18:01:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-19 18:01:09,699] {spark_submit.py:490} INFO - 23/05/19 18:01:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 207.4 KiB, free 434.2 MiB)
[2023-05-19 18:01:10,193] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 18:01:10,214] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.15.8:54977 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:01:10,230] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:10,256] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:01:10,748] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:10,797] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:01:10,798] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:01:10,798] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:01:10,800] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:01:10,810] {spark_submit.py:490} INFO - 23/05/19 18:01:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:01:11,110] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.1 MiB)
[2023-05-19 18:01:11,130] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.1 MiB)
[2023-05-19 18:01:11,132] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.15.8:54977 (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 18:01:11,136] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:01:11,174] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:01:11,178] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-19 18:01:11,311] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5003 bytes) taskResourceAssignments Map()
[2023-05-19 18:01:11,347] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-19 18:01:11,606] {spark_submit.py:490} INFO - 23/05/19 18:01:11 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4504, partition values: [empty row]
[2023-05-19 18:01:12,263] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO CodeGenerator: Code generated in 567.865619 ms
[2023-05-19 18:01:12,469] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-19 18:01:12,501] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1210 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:01:12,503] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-19 18:01:12,523] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.662 s
[2023-05-19 18:01:12,524] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:01:12,525] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-19 18:01:12,532] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.785431 s
[2023-05-19 18:01:12,953] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.15.8:54977 in memory (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:01:12,964] {spark_submit.py:490} INFO - 23/05/19 18:01:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.15.8:54977 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 18:01:13,638] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-19 18:01:13,642] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-19 18:01:13,643] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-19 18:01:13,894] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:01:13,895] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:01:13,901] {spark_submit.py:490} INFO - 23/05/19 18:01:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:01:14,553] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO CodeGenerator: Code generated in 297.751197 ms
[2023-05-19 18:01:14,609] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 207.3 KiB, free 434.2 MiB)
[2023-05-19 18:01:14,778] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 18:01:14,779] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.15.8:54977 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:01:14,799] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:14,858] {spark_submit.py:490} INFO - 23/05/19 18:01:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:01:15,163] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:15,174] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:01:15,176] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:01:15,177] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:01:15,177] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:01:15,178] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:01:15,293] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 235.7 KiB, free 433.9 MiB)
[2023-05-19 18:01:15,326] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.5 KiB, free 433.9 MiB)
[2023-05-19 18:01:15,328] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.15.8:54977 (size: 82.5 KiB, free: 434.3 MiB)
[2023-05-19 18:01:15,329] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:01:15,331] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:01:15,332] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-19 18:01:15,343] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5232 bytes) taskResourceAssignments Map()
[2023-05-19 18:01:15,344] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-19 18:01:15,460] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:01:15,461] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:01:15,462] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:01:15,665] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4504, partition values: [empty row]
[2023-05-19 18:01:15,932] {spark_submit.py:490} INFO - 23/05/19 18:01:15 INFO CodeGenerator: Code generated in 238.97193 ms
[2023-05-19 18:01:16,044] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO CodeGenerator: Code generated in 15.981015 ms
[2023-05-19 18:01:16,180] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileOutputCommitter: Saved output of task 'attempt_202305191801146024455776262977500_0001_m_000000_1' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/tweet/process_date=2023-05-18/_temporary/0/task_202305191801146024455776262977500_0001_m_000000
[2023-05-19 18:01:16,183] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO SparkHadoopMapRedUtil: attempt_202305191801146024455776262977500_0001_m_000000_1: Committed. Elapsed time: 4 ms.
[2023-05-19 18:01:16,217] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-19 18:01:16,222] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 883 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:01:16,223] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-19 18:01:16,224] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 1.045 s
[2023-05-19 18:01:16,225] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:01:16,226] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-19 18:01:16,227] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 1.061932 s
[2023-05-19 18:01:16,234] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileFormatWriter: Start to commit write Job e48e5220-b2bf-4988-9f07-63dd9a444831.
[2023-05-19 18:01:16,279] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileFormatWriter: Write Job e48e5220-b2bf-4988-9f07-63dd9a444831 committed. Elapsed time: 45 ms.
[2023-05-19 18:01:16,283] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileFormatWriter: Finished processing stats for write job e48e5220-b2bf-4988-9f07-63dd9a444831.
[2023-05-19 18:01:16,385] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 18:01:16,386] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 18:01:16,387] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-19 18:01:16,443] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:01:16,444] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:01:16,445] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:01:16,542] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO CodeGenerator: Code generated in 23.396676 ms
[2023-05-19 18:01:16,552] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 207.3 KiB, free 433.7 MiB)
[2023-05-19 18:01:16,614] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.15.8:54977 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-19 18:01:16,619] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.15.8:54977 in memory (size: 82.5 KiB, free: 434.4 MiB)
[2023-05-19 18:01:16,667] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 18:01:16,669] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.15.8:54977 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:01:16,675] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:16,679] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198808 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:01:16,928] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:01:16,930] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:01:16,931] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:01:16,933] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:01:16,934] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:01:16,935] {spark_submit.py:490} INFO - 23/05/19 18:01:16 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:01:17,021] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 215.8 KiB, free 434.0 MiB)
[2023-05-19 18:01:17,036] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-05-19 18:01:17,042] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.15.8:54977 (size: 77.2 KiB, free: 434.3 MiB)
[2023-05-19 18:01:17,044] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:01:17,047] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:01:17,048] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-19 18:01:17,050] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5232 bytes) taskResourceAssignments Map()
[2023-05-19 18:01:17,051] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-19 18:01:17,101] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:01:17,102] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:01:17,103] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:01:17,180] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-4504, partition values: [empty row]
[2023-05-19 18:01:17,219] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO CodeGenerator: Code generated in 30.026228 ms
[2023-05-19 18:01:17,284] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileOutputCommitter: Saved output of task 'attempt_20230519180116524436968489210575_0002_m_000000_2' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/user/process_date=2023-05-18/_temporary/0/task_20230519180116524436968489210575_0002_m_000000
[2023-05-19 18:01:17,285] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SparkHadoopMapRedUtil: attempt_20230519180116524436968489210575_0002_m_000000_2: Committed. Elapsed time: 48 ms.
[2023-05-19 18:01:17,288] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-19 18:01:17,293] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 242 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:01:17,294] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-19 18:01:17,296] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.359 s
[2023-05-19 18:01:17,299] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:01:17,300] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-19 18:01:17,301] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.371166 s
[2023-05-19 18:01:17,302] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileFormatWriter: Start to commit write Job a45e4cf6-06bc-472e-b738-1aec0f283699.
[2023-05-19 18:01:17,355] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileFormatWriter: Write Job a45e4cf6-06bc-472e-b738-1aec0f283699 committed. Elapsed time: 53 ms.
[2023-05-19 18:01:17,358] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO FileFormatWriter: Finished processing stats for write job a45e4cf6-06bc-472e-b738-1aec0f283699.
[2023-05-19 18:01:17,434] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-19 18:01:17,467] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SparkUI: Stopped Spark web UI at http://192.168.15.8:4041
[2023-05-19 18:01:17,500] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-19 18:01:17,532] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO MemoryStore: MemoryStore cleared
[2023-05-19 18:01:17,533] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO BlockManager: BlockManager stopped
[2023-05-19 18:01:17,540] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-19 18:01:17,547] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-19 18:01:17,569] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO SparkContext: Successfully stopped SparkContext
[2023-05-19 18:01:17,572] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO ShutdownHookManager: Shutdown hook called
[2023-05-19 18:01:17,573] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-65e5f7db-d13a-4bc7-bf7a-60f14bb8be9c
[2023-05-19 18:01:17,588] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-b4ddef01-5260-47a6-9139-e88b14699f29
[2023-05-19 18:01:17,603] {spark_submit.py:490} INFO - 23/05/19 18:01:17 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-65e5f7db-d13a-4bc7-bf7a-60f14bb8be9c/pyspark-3c6a70a6-f385-41bf-a055-540f3d752abf
[2023-05-19 18:01:17,729] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230518T000000, start_date=20230519T210045, end_date=20230519T210117
[2023-05-19 18:01:17,790] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-19 18:01:17,836] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-19 18:24:42,086] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 18:24:42,105] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [queued]>
[2023-05-19 18:24:42,105] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 18:24:42,105] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-19 18:24:42,105] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-19 18:24:42,127] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-18 00:00:00+00:00
[2023-05-19 18:24:42,131] {standard_task_runner.py:52} INFO - Started process 9783 to run task
[2023-05-19 18:24:42,138] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-18T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmp_smmeb4r', '--error-file', '/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/tmpmzlubbt8']
[2023-05-19 18:24:42,141] {standard_task_runner.py:80} INFO - Job 46: Subtask transform_twitter_datascience
[2023-05-19 18:24:42,215] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-18T00:00:00+00:00 [running]> on host 1.0.0.127.in-addr.arpa
[2023-05-19 18:24:42,712] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-18T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-18T00:00:00+00:00
[2023-05-19 18:24:42,736] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-19 18:24:42,757] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /Users/marianaoliveira/Downloads/alura1/src/spark/transformation.py --src /Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18 --dest /Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/ --process-date 2023-05-18
[2023-05-19 18:24:48,050] {spark_submit.py:490} INFO - 23/05/19 18:24:48 WARN Utils: Your hostname, MacBook-Air-de-Mariana-2.local resolves to a loopback address: 127.0.0.1; using 192.168.15.8 instead (on interface en0)
[2023-05-19 18:24:48,064] {spark_submit.py:490} INFO - 23/05/19 18:24:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-19 18:24:51,128] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO SparkContext: Running Spark version 3.3.2
[2023-05-19 18:24:51,337] {spark_submit.py:490} INFO - 23/05/19 18:24:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-19 18:24:51,669] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceUtils: ==============================================================
[2023-05-19 18:24:51,670] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-19 18:24:51,677] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceUtils: ==============================================================
[2023-05-19 18:24:51,678] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-19 18:24:51,743] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-19 18:24:51,834] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceProfile: Limiting resource is cpu
[2023-05-19 18:24:51,843] {spark_submit.py:490} INFO - 23/05/19 18:24:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-19 18:24:52,003] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SecurityManager: Changing view acls to: marianaoliveira
[2023-05-19 18:24:52,005] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SecurityManager: Changing modify acls to: marianaoliveira
[2023-05-19 18:24:52,007] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SecurityManager: Changing view acls groups to:
[2023-05-19 18:24:52,009] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SecurityManager: Changing modify acls groups to:
[2023-05-19 18:24:52,010] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marianaoliveira); groups with view permissions: Set(); users  with modify permissions: Set(marianaoliveira); groups with modify permissions: Set()
[2023-05-19 18:24:52,909] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO Utils: Successfully started service 'sparkDriver' on port 55883.
[2023-05-19 18:24:52,999] {spark_submit.py:490} INFO - 23/05/19 18:24:52 INFO SparkEnv: Registering MapOutputTracker
[2023-05-19 18:24:53,094] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-19 18:24:53,147] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-19 18:24:53,147] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-19 18:24:53,164] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-19 18:24:53,229] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO DiskBlockManager: Created local directory at /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/blockmgr-d9637d8d-0fdc-4f24-be89-cfb6a90a36c9
[2023-05-19 18:24:53,272] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-05-19 18:24:53,310] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-19 18:24:53,913] {spark_submit.py:490} INFO - 23/05/19 18:24:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-05-19 18:24:53,937] {spark_submit.py:490} INFO - 23/05/19 18:24:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-05-19 18:24:54,296] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO Executor: Starting executor ID driver on host 192.168.15.8
[2023-05-19 18:24:54,313] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-19 18:24:54,367] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55884.
[2023-05-19 18:24:54,368] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO NettyBlockTransferService: Server created on 192.168.15.8:55884
[2023-05-19 18:24:54,375] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-19 18:24:54,392] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.15.8, 55884, None)
[2023-05-19 18:24:54,399] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.15.8:55884 with 434.4 MiB RAM, BlockManagerId(driver, 192.168.15.8, 55884, None)
[2023-05-19 18:24:54,407] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.15.8, 55884, None)
[2023-05-19 18:24:54,409] {spark_submit.py:490} INFO - 23/05/19 18:24:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.15.8, 55884, None)
[2023-05-19 18:24:55,887] {spark_submit.py:490} INFO - 23/05/19 18:24:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-19 18:24:55,913] {spark_submit.py:490} INFO - 23/05/19 18:24:55 INFO SharedState: Warehouse path is 'file:/Users/marianaoliveira/Downloads/alura1/spark-warehouse'.
[2023-05-19 18:24:58,110] {spark_submit.py:490} INFO - 23/05/19 18:24:58 INFO InMemoryFileIndex: It took 116 ms to list leaf files for 1 paths.
[2023-05-19 18:24:58,301] {spark_submit.py:490} INFO - 23/05/19 18:24:58 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
[2023-05-19 18:25:03,900] {spark_submit.py:490} INFO - 23/05/19 18:25:03 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 18:25:03,905] {spark_submit.py:490} INFO - 23/05/19 18:25:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 18:25:03,911] {spark_submit.py:490} INFO - 23/05/19 18:25:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-19 18:25:04,693] {spark_submit.py:490} INFO - 23/05/19 18:25:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 207.4 KiB, free 434.2 MiB)
[2023-05-19 18:25:05,102] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
[2023-05-19 18:25:05,108] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.15.8:55884 (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 18:25:05,117] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:05,145] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212480 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:25:05,648] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:05,679] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:25:05,680] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:25:05,680] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:25:05,683] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:25:05,689] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:25:05,897] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 434.1 MiB)
[2023-05-19 18:25:05,914] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.1 MiB)
[2023-05-19 18:25:05,915] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.15.8:55884 (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 18:25:05,916] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:25:05,949] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:25:05,951] {spark_submit.py:490} INFO - 23/05/19 18:25:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-19 18:25:06,058] {spark_submit.py:490} INFO - 23/05/19 18:25:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5003 bytes) taskResourceAssignments Map()
[2023-05-19 18:25:06,086] {spark_submit.py:490} INFO - 23/05/19 18:25:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-19 18:25:06,361] {spark_submit.py:490} INFO - 23/05/19 18:25:06 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-18176, partition values: [empty row]
[2023-05-19 18:25:06,963] {spark_submit.py:490} INFO - 23/05/19 18:25:06 INFO CodeGenerator: Code generated in 514.60003 ms
[2023-05-19 18:25:07,203] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-05-19 18:25:07,218] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1182 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:25:07,222] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-19 18:25:07,236] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.497 s
[2023-05-19 18:25:07,237] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:25:07,238] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-19 18:25:07,243] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.591054 s
[2023-05-19 18:25:07,581] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.15.8:55884 in memory (size: 34.4 KiB, free: 434.4 MiB)
[2023-05-19 18:25:07,589] {spark_submit.py:490} INFO - 23/05/19 18:25:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.15.8:55884 in memory (size: 7.0 KiB, free: 434.4 MiB)
[2023-05-19 18:25:08,381] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-19 18:25:08,385] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-19 18:25:08,387] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-19 18:25:08,803] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:25:08,804] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:25:08,807] {spark_submit.py:490} INFO - 23/05/19 18:25:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:25:09,510] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO CodeGenerator: Code generated in 330.444897 ms
[2023-05-19 18:25:09,537] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 207.3 KiB, free 434.2 MiB)
[2023-05-19 18:25:09,608] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 18:25:09,610] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.15.8:55884 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:25:09,612] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:09,617] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212480 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:25:09,868] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:09,872] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:25:09,873] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:25:09,874] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:25:09,875] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:25:09,875] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:25:09,975] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 235.7 KiB, free 433.9 MiB)
[2023-05-19 18:25:09,990] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 82.5 KiB, free 433.9 MiB)
[2023-05-19 18:25:09,992] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.15.8:55884 (size: 82.5 KiB, free: 434.3 MiB)
[2023-05-19 18:25:09,995] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:25:09,996] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:25:09,996] {spark_submit.py:490} INFO - 23/05/19 18:25:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-19 18:25:10,003] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5232 bytes) taskResourceAssignments Map()
[2023-05-19 18:25:10,004] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-19 18:25:10,117] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:25:10,118] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:25:10,119] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:25:10,267] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-18176, partition values: [empty row]
[2023-05-19 18:25:10,365] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO CodeGenerator: Code generated in 86.713315 ms
[2023-05-19 18:25:10,417] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO CodeGenerator: Code generated in 8.7621 ms
[2023-05-19 18:25:10,550] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileOutputCommitter: Saved output of task 'attempt_202305191825096097983089957437741_0001_m_000000_1' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/tweet/process_date=2023-05-18/_temporary/0/task_202305191825096097983089957437741_0001_m_000000
[2023-05-19 18:25:10,552] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO SparkHadoopMapRedUtil: attempt_202305191825096097983089957437741_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-05-19 18:25:10,568] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-19 18:25:10,571] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 573 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:25:10,572] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-19 18:25:10,574] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.698 s
[2023-05-19 18:25:10,576] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:25:10,576] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-19 18:25:10,578] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.707824 s
[2023-05-19 18:25:10,581] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileFormatWriter: Start to commit write Job ee918da9-682e-4840-9fb2-bc2408d6516e.
[2023-05-19 18:25:10,626] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileFormatWriter: Write Job ee918da9-682e-4840-9fb2-bc2408d6516e committed. Elapsed time: 44 ms.
[2023-05-19 18:25:10,633] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileFormatWriter: Finished processing stats for write job ee918da9-682e-4840-9fb2-bc2408d6516e.
[2023-05-19 18:25:10,710] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileSourceStrategy: Pushed Filters:
[2023-05-19 18:25:10,710] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-19 18:25:10,711] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-19 18:25:10,740] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:25:10,741] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:25:10,743] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:25:10,813] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO CodeGenerator: Code generated in 21.482751 ms
[2023-05-19 18:25:10,825] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 207.3 KiB, free 433.7 MiB)
[2023-05-19 18:25:10,851] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.15.8:55884 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2023-05-19 18:25:10,859] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.15.8:55884 in memory (size: 82.5 KiB, free: 434.4 MiB)
[2023-05-19 18:25:10,893] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2023-05-19 18:25:10,895] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.15.8:55884 (size: 34.3 KiB, free: 434.4 MiB)
[2023-05-19 18:25:10,900] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:10,903] {spark_submit.py:490} INFO - 23/05/19 18:25:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4212480 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-19 18:25:11,045] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-19 18:25:11,048] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-19 18:25:11,048] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-19 18:25:11,049] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Parents of final stage: List()
[2023-05-19 18:25:11,050] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Missing parents: List()
[2023-05-19 18:25:11,051] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-19 18:25:11,144] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 215.8 KiB, free 434.0 MiB)
[2023-05-19 18:25:11,156] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-05-19 18:25:11,158] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.15.8:55884 (size: 77.2 KiB, free: 434.3 MiB)
[2023-05-19 18:25:11,161] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-19 18:25:11,163] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-19 18:25:11,163] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-19 18:25:11,166] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.15.8, executor driver, partition 0, PROCESS_LOCAL, 5232 bytes) taskResourceAssignments Map()
[2023-05-19 18:25:11,167] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-19 18:25:11,218] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-19 18:25:11,219] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-19 18:25:11,219] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-19 18:25:11,283] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileScanRDD: Reading File path: file:///Users/marianaoliveira/Downloads/alura1/datalake/bronze/twitter_datascience/extract_date=2023-05-18/datascience_20230518.json, range: 0-18176, partition values: [empty row]
[2023-05-19 18:25:11,316] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO CodeGenerator: Code generated in 26.767783 ms
[2023-05-19 18:25:11,337] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileOutputCommitter: Saved output of task 'attempt_202305191825105574880324027671492_0002_m_000000_2' to file:/Users/marianaoliveira/Downloads/alura1/datalake/silver/twitter_datascience/user/process_date=2023-05-18/_temporary/0/task_202305191825105574880324027671492_0002_m_000000
[2023-05-19 18:25:11,338] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkHadoopMapRedUtil: attempt_202305191825105574880324027671492_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-05-19 18:25:11,343] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-19 18:25:11,346] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 180 ms on 192.168.15.8 (executor driver) (1/1)
[2023-05-19 18:25:11,346] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-19 18:25:11,348] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.297 s
[2023-05-19 18:25:11,349] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-19 18:25:11,350] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-19 18:25:11,351] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.305067 s
[2023-05-19 18:25:11,352] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileFormatWriter: Start to commit write Job 4a67c357-7fbc-4cc5-99e2-be81f042a123.
[2023-05-19 18:25:11,399] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileFormatWriter: Write Job 4a67c357-7fbc-4cc5-99e2-be81f042a123 committed. Elapsed time: 47 ms.
[2023-05-19 18:25:11,400] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO FileFormatWriter: Finished processing stats for write job 4a67c357-7fbc-4cc5-99e2-be81f042a123.
[2023-05-19 18:25:11,488] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-19 18:25:11,524] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkUI: Stopped Spark web UI at http://192.168.15.8:4041
[2023-05-19 18:25:11,546] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-19 18:25:11,568] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO MemoryStore: MemoryStore cleared
[2023-05-19 18:25:11,569] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO BlockManager: BlockManager stopped
[2023-05-19 18:25:11,575] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-19 18:25:11,581] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-19 18:25:11,599] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO SparkContext: Successfully stopped SparkContext
[2023-05-19 18:25:11,600] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO ShutdownHookManager: Shutdown hook called
[2023-05-19 18:25:11,601] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-bb8a3d23-b84b-4b21-8235-abecd99f3b62
[2023-05-19 18:25:11,621] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-548e998d-9419-4e35-a348-b58de2277a78
[2023-05-19 18:25:11,633] {spark_submit.py:490} INFO - 23/05/19 18:25:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/4m/2vllbq951394clq6jtlsj0p40000gn/T/spark-bb8a3d23-b84b-4b21-8235-abecd99f3b62/pyspark-0742ed15-5d09-414a-9efa-445cb914a278
[2023-05-19 18:25:11,754] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230518T000000, start_date=20230519T212442, end_date=20230519T212511
[2023-05-19 18:25:11,812] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-19 18:25:11,846] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
